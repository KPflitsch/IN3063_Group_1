{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T11:33:01.156523Z",
     "start_time": "2024-12-29T11:32:15.129420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from task_1.core_NN_implementation.fullyConnetedNN import fullyConnectedNN\n",
    "from task_1.core_NN_implementation.datasetHelper import load_preprocess_cifar10\n",
    "from task_1.Optimiser.optimiser import SGD, SGDWithMomentum\n",
    "\n",
    "\n",
    "data_dir = \"../dataset/cifar-10-batches-py/\"\n",
    "optimiser = SGDWithMomentum(learning_rate=0.001, momentum=0.9)\n",
    "#optimiser = SGD(learning_rate=0.001)\n",
    "\n",
    "# Load and preprocess cifar 10 dataset\n",
    "X_train, y_train, X_test, y_test = load_preprocess_cifar10(data_dir)\n",
    "\n",
    "#parameters for the neural network\n",
    "hidden_layers = [512, 256]\n",
    "activations = ['relu', 'relu', 'softmax']\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "regularization = 'L2'\n",
    "\n",
    "#making the neural network\n",
    "model = fullyConnectedNN(\n",
    "    input_size=3072,\n",
    "    output_size=10,\n",
    "    hidden_layers=hidden_layers,\n",
    "    activations=activations,\n",
    "    learning_rate=learning_rate,\n",
    "    dropout_rate=dropout_rate,\n",
    "    regularization=regularization,\n",
    "    optimiser = optimiser\n",
    ")\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "#Train the Model\n",
    "history = model.train(\n",
    "    X_train_flat, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    X_val=X_test_flat, y_val=y_test\n",
    ")\n",
    "\n",
    "#Evaluate on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#Plot Training and Validation Metrics\n",
    "model.plot_metrics(history)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3.4881, Accuracy: 0.0999, Val Loss: 3.3863, Val Accuracy: 0.1000, Learning Rate: 0.0010\n",
      "Epoch 2/50, Loss: 3.3723, Accuracy: 0.1000, Val Loss: 3.3605, Val Accuracy: 0.1000, Learning Rate: 0.0010\n",
      "Epoch 3/50, Loss: 3.3450, Accuracy: 0.1000, Val Loss: 3.3280, Val Accuracy: 0.1000, Learning Rate: 0.0010\n",
      "Epoch 4/50, Loss: 3.3128, Accuracy: 0.1000, Val Loss: 3.2963, Val Accuracy: 0.1000, Learning Rate: 0.0010\n",
      "Epoch 5/50, Loss: 3.2817, Accuracy: 0.1000, Val Loss: 3.2656, Val Accuracy: 0.1000, Learning Rate: 0.0010\n",
      "Epoch 6/50, Loss: 3.2514, Accuracy: 0.1000, Val Loss: 3.2359, Val Accuracy: 0.1000, Learning Rate: 0.0010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 36\u001B[0m\n\u001B[0;32m     33\u001B[0m X_test_flat \u001B[38;5;241m=\u001B[39m X_test\u001B[38;5;241m.\u001B[39mreshape(X_test\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m#Train the Model\u001B[39;00m\n\u001B[1;32m---> 36\u001B[0m history \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtrain(\n\u001B[0;32m     37\u001B[0m     X_train_flat, y_train,\n\u001B[0;32m     38\u001B[0m     epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m,\n\u001B[0;32m     39\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m,\n\u001B[0;32m     40\u001B[0m     X_val\u001B[38;5;241m=\u001B[39mX_test_flat, y_val\u001B[38;5;241m=\u001B[39my_test\n\u001B[0;32m     41\u001B[0m )\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m#Evaluate on Test Data\u001B[39;00m\n\u001B[0;32m     44\u001B[0m test_loss, test_accuracy \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate(X_test_flat, y_test)\n",
      "File \u001B[1;32m~\\Desktop\\IN3063_Group_1\\task_1\\core_NN_implementation\\fullyConnetedNN.py:199\u001B[0m, in \u001B[0;36mfullyConnectedNN.train\u001B[1;34m(self, X, y, epochs, batch_size, X_val, y_val)\u001B[0m\n\u001B[0;32m    196\u001B[0m     epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n\u001B[0;32m    198\u001B[0m     \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n\u001B[1;32m--> 199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackward_propagation(x_batch, y_batch)\n\u001B[0;32m    201\u001B[0m     num_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;66;03m# Average loss for the epoch\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\IN3063_Group_1\\task_1\\core_NN_implementation\\fullyConnetedNN.py:135\u001B[0m, in \u001B[0;36mfullyConnectedNN.backward_propagation\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mset\u001B[39m(weight_dict\u001B[38;5;241m.\u001B[39mkeys()) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mset\u001B[39m(gradients\u001B[38;5;241m.\u001B[39mkeys()):\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKeys in weight_dict and gradients do not match.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 135\u001B[0m updated_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimiser\u001B[38;5;241m.\u001B[39mupdate(weight_dict, gradients)\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(updated_weights)):\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights[i] \u001B[38;5;241m=\u001B[39m updated_weights[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlayer_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mW\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32m~\\Desktop\\IN3063_Group_1\\task_1\\Optimiser\\optimiser.py:40\u001B[0m, in \u001B[0;36mSGDWithMomentum.update\u001B[1;34m(self, weights, gradients)\u001B[0m\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvelocities[layer][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdb\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     35\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmomentum \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvelocities[layer][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdb\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m\n\u001B[0;32m     36\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rate \u001B[38;5;241m*\u001B[39m gradients[layer][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdb\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     37\u001B[0m     )\n\u001B[0;32m     39\u001B[0m     weights[layer][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mW\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvelocities[layer][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdW\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m---> 40\u001B[0m     weights[layer][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvelocities[layer][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdb\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m weights\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Plot learing rate\n",
    "model.plot_learning_rate(history['learning_rate'])\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#parameters for the neural network with increase dropout\n",
    "hidden_layers = [512, 256]\n",
    "activations = ['relu', 'relu', 'softmax']\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.8\n",
    "regularization = 'L2'\n",
    "\n",
    "#making the neural network\n",
    "model = fullyConnectedNN(\n",
    "    input_size=3072,\n",
    "    output_size=10,\n",
    "    hidden_layers=hidden_layers,\n",
    "    activations=activations,\n",
    "    learning_rate=learning_rate,\n",
    "    dropout_rate=dropout_rate,\n",
    "    regularization=regularization,\n",
    "    optimiser = optimiser\n",
    ")\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "#Train the Model\n",
    "history = model.train(\n",
    "    X_train_flat, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    X_val=X_test_flat, y_val=y_test\n",
    ")\n",
    "\n",
    "#Evaluate on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#Plot Training and Validation Metrics\n",
    "model.plot_metrics(history)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Plot learing rate\n",
    "model.plot_learning_rate(history['learning_rate'])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#parameters for the neural network with decrease dropout\n",
    "hidden_layers = [512, 256]\n",
    "activations = ['relu', 'relu', 'softmax']\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.2\n",
    "regularization = 'L2'\n",
    "\n",
    "#making the neural network\n",
    "model = fullyConnectedNN(\n",
    "    input_size=3072,\n",
    "    output_size=10,\n",
    "    hidden_layers=hidden_layers,\n",
    "    activations=activations,\n",
    "    learning_rate=learning_rate,\n",
    "    dropout_rate=dropout_rate,\n",
    "    regularization=regularization,\n",
    "    optimiser = optimiser\n",
    ")\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "#Train the Model\n",
    "history = model.train(\n",
    "    X_train_flat, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    X_val=X_test_flat, y_val=y_test\n",
    ")\n",
    "\n",
    "#Evaluate on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#Plot Training and Validation Metrics\n",
    "model.plot_metrics(history)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Plot learing rate\n",
    "model.plot_learning_rate(history['learning_rate'])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#parameters for the neural network with increased learning rate\n",
    "hidden_layers = [512, 256]\n",
    "activations = ['relu', 'relu', 'softmax']\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.5\n",
    "regularization = 'L2'\n",
    "\n",
    "#making the neural network\n",
    "model = fullyConnectedNN(\n",
    "    input_size=3072,\n",
    "    output_size=10,\n",
    "    hidden_layers=hidden_layers,\n",
    "    activations=activations,\n",
    "    learning_rate=learning_rate,\n",
    "    dropout_rate=dropout_rate,\n",
    "    regularization=regularization,\n",
    "    optimiser = optimiser\n",
    ")\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "#Train the Model\n",
    "history = model.train(\n",
    "    X_train_flat, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    X_val=X_test_flat, y_val=y_test\n",
    ")\n",
    "\n",
    "#Evaluate on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#Plot Training and Validation Metrics\n",
    "model.plot_metrics(history)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Plot learing rate\n",
    "model.plot_learning_rate(history['learning_rate'])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#parameters for the neural network with decreased learning rate\n",
    "hidden_layers = [512, 256]\n",
    "activations = ['relu', 'relu', 'softmax']\n",
    "learning_rate = 0.0001\n",
    "dropout_rate = 0.5\n",
    "regularization = 'L2'\n",
    "\n",
    "#making the neural network\n",
    "model = fullyConnectedNN(\n",
    "    input_size=3072,\n",
    "    output_size=10,\n",
    "    hidden_layers=hidden_layers,\n",
    "    activations=activations,\n",
    "    learning_rate=learning_rate,\n",
    "    dropout_rate=dropout_rate,\n",
    "    regularization=regularization,\n",
    "    optimiser = optimiser\n",
    ")\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "#Train the Model\n",
    "history = model.train(\n",
    "    X_train_flat, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    X_val=X_test_flat, y_val=y_test\n",
    ")\n",
    "\n",
    "#Evaluate on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#Plot Training and Validation Metrics\n",
    "model.plot_metrics(history)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Plot learing rate\n",
    "model.plot_learning_rate(history['learning_rate'])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#parameters for the neural network with L1 regularization\n",
    "hidden_layers = [512, 256]\n",
    "activations = ['relu', 'relu', 'softmax']\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "regularization = 'L1'\n",
    "\n",
    "#making the neural network\n",
    "model = fullyConnectedNN(\n",
    "    input_size=3072,\n",
    "    output_size=10,\n",
    "    hidden_layers=hidden_layers,\n",
    "    activations=activations,\n",
    "    learning_rate=learning_rate,\n",
    "    dropout_rate=dropout_rate,\n",
    "    regularization=regularization,\n",
    "    optimiser = optimiser\n",
    ")\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "#Train the Model\n",
    "history = model.train(\n",
    "    X_train_flat, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    X_val=X_test_flat, y_val=y_test\n",
    ")\n",
    "\n",
    "#Evaluate on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#Plot Training and Validation Metrics\n",
    "model.plot_metrics(history)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Plot learing rate\n",
    "model.plot_learning_rate(history['learning_rate'])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#parameters for the neural network\n",
    "hidden_layers = [512, 256]\n",
    "activations = ['relu', 'relu', 'softmax']\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "regularization = 'L2'\n",
    "\n",
    "#making the neural network\n",
    "model = fullyConnectedNN(\n",
    "    input_size=3072,\n",
    "    output_size=10,\n",
    "    hidden_layers=hidden_layers,\n",
    "    activations=activations,\n",
    "    learning_rate=learning_rate,\n",
    "    dropout_rate=dropout_rate,\n",
    "    regularization=regularization,\n",
    "    optimiser = optimiser\n",
    ")\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "#Train the Model with  smaller batch size\n",
    "history = model.train(\n",
    "    X_train_flat, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    X_val=X_test_flat, y_val=y_test\n",
    ")\n",
    "\n",
    "#Evaluate on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#Plot Training and Validation Metrics\n",
    "model.plot_metrics(history)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Plot learing rate\n",
    "model.plot_learning_rate(history['learning_rate'])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#parameters for the neural network\n",
    "hidden_layers = [512, 256]\n",
    "activations = ['relu', 'relu', 'softmax']\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "regularization = 'L2'\n",
    "\n",
    "#making the neural network\n",
    "model = fullyConnectedNN(\n",
    "    input_size=3072,\n",
    "    output_size=10,\n",
    "    hidden_layers=hidden_layers,\n",
    "    activations=activations,\n",
    "    learning_rate=learning_rate,\n",
    "    dropout_rate=dropout_rate,\n",
    "    regularization=regularization,\n",
    "    optimiser = optimiser\n",
    ")\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "#Train the Model with bigger batch size\n",
    "history = model.train(\n",
    "    X_train_flat, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    X_val=X_test_flat, y_val=y_test\n",
    ")\n",
    "\n",
    "#Evaluate on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#Plot Training and Validation Metrics\n",
    "model.plot_metrics(history)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Plot learing rate\n",
    "model.plot_learning_rate(history['learning_rate'])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#parameters for the neural network\n",
    "hidden_layers = [512, 256]\n",
    "activations = ['relu', 'relu', 'softmax']\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "regularization = 'L2'\n",
    "\n",
    "#making the neural network\n",
    "model = fullyConnectedNN(\n",
    "    input_size=3072,\n",
    "    output_size=10,\n",
    "    hidden_layers=hidden_layers,\n",
    "    activations=activations,\n",
    "    learning_rate=learning_rate,\n",
    "    dropout_rate=dropout_rate,\n",
    "    regularization=regularization,\n",
    "    optimiser = optimiser\n",
    ")\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "#Train the Model with less epochs\n",
    "history = model.train(\n",
    "    X_train_flat, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    X_val=X_test_flat, y_val=y_test\n",
    ")\n",
    "\n",
    "#Evaluate on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#Plot Training and Validation Metrics\n",
    "model.plot_metrics(history)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Plot learing rate\n",
    "model.plot_learning_rate(history['learning_rate'])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#parameters for the neural network\n",
    "hidden_layers = [512, 256]\n",
    "activations = ['relu', 'relu', 'softmax']\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "regularization = 'L2'\n",
    "\n",
    "#making the neural network\n",
    "model = fullyConnectedNN(\n",
    "    input_size=3072,\n",
    "    output_size=10,\n",
    "    hidden_layers=hidden_layers,\n",
    "    activations=activations,\n",
    "    learning_rate=learning_rate,\n",
    "    dropout_rate=dropout_rate,\n",
    "    regularization=regularization,\n",
    "    optimiser = optimiser\n",
    ")\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "#Train the Model with more epochs\n",
    "history = model.train(\n",
    "    X_train_flat, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    X_val=X_test_flat, y_val=y_test\n",
    ")\n",
    "\n",
    "#Evaluate on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#Plot Training and Validation Metrics\n",
    "model.plot_metrics(history)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Plot learing rate\n",
    "model.plot_learning_rate(history['learning_rate'])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#parameters for the neural network if the activation layer is none\n",
    "hidden_layers = [512, 256]\n",
    "activations = None\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "regularization = 'L2'\n",
    "\n",
    "#making the neural network\n",
    "model = fullyConnectedNN(\n",
    "    input_size=3072,\n",
    "    output_size=10,\n",
    "    hidden_layers=hidden_layers,\n",
    "    activations=activations,\n",
    "    learning_rate=learning_rate,\n",
    "    dropout_rate=dropout_rate,\n",
    "    regularization=regularization,\n",
    "    optimiser = optimiser\n",
    ")\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "#Train the Model with more epochs\n",
    "history = model.train(\n",
    "    X_train_flat, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    X_val=X_test_flat, y_val=y_test\n",
    ")\n",
    "\n",
    "#Evaluate on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#Plot Training and Validation Metrics\n",
    "model.plot_metrics(history)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Plot learing rate\n",
    "model.plot_learning_rate(history['learning_rate'])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#parameters for the neural network if the activation layer is just relu layer\n",
    "hidden_layers = [512, 256]\n",
    "activations = ['relu','relu']\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "regularization = 'L2'\n",
    "\n",
    "#making the neural network\n",
    "model = fullyConnectedNN(\n",
    "    input_size=3072,\n",
    "    output_size=10,\n",
    "    hidden_layers=hidden_layers,\n",
    "    activations=activations,\n",
    "    learning_rate=learning_rate,\n",
    "    dropout_rate=dropout_rate,\n",
    "    regularization=regularization,\n",
    "    optimiser = optimiser\n",
    ")\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "#Train the Model with more epochs\n",
    "history = model.train(\n",
    "    X_train_flat, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    X_val=X_test_flat, y_val=y_test\n",
    ")\n",
    "\n",
    "#Evaluate on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#Plot Training and Validation Metrics\n",
    "model.plot_metrics(history)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Plot learing rate\n",
    "model.plot_learning_rate(history['learning_rate'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
